{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/anas/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation,Dropout\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_files\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  22564\n",
      "Testing set size :  2513\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/Users/anas/Desktop/waste-classification-data/DATASET/TRAIN'\n",
    "test_dir = '/Users/anas/Desktop/waste-classification-data/DATASET/TEST'\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path) #load all files from the path\n",
    "    files = np.array(data['filenames']) #get the file  \n",
    "    targets = np.array(data['target'])#get the the classification labels as integer index\n",
    "    target_labels = np.array(data['target_names'])#get the the classification labels \n",
    "    return files,targets,target_labels\n",
    "    \n",
    "x_train, y_train,target_labels = load_dataset(train_dir)\n",
    "x_test, y_test,_ = load_dataset(test_dir)\n",
    "\n",
    "print('Training set size : ' , x_train.shape[0])\n",
    "print('Testing set size : ', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (18051,)\n",
      "y_train shape: (18051,)\n",
      "x_validate shape: (4513,)\n",
      "y_validate shape: (4513,)\n",
      "x_test shape: (2513,)\n",
      "y_test shape: (2513,)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train shape: \" + str(x_train.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"x_validate shape: \" + str(x_validate.shape))\n",
    "print (\"y_validate shape: \" + str(y_validate.shape))\n",
    "print (\"x_test shape: \" + str(x_test.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape :  (18051, 100, 100, 3)\n",
      "Validation set shape :  (4513, 100, 100, 3)\n",
      "Test set shape :  (2513, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "def convert_image_to_array(files):\n",
    "    width, height, channels = 100, 100, 3\n",
    "    images_as_array = np.empty((files.shape[0], width, height, channels), dtype=np.uint8) #define train and test data shape\n",
    "    for idx,file in enumerate(files):\n",
    "        img = cv2.imread(file) \n",
    "        res = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_CUBIC) #As images have different size, resizing all images to have same shape of image array\n",
    "        images_as_array[idx] = res\n",
    "    return images_as_array\n",
    "\n",
    "x_train = np.array(convert_image_to_array(x_train))\n",
    "print('Training set shape : ',x_train.shape)\n",
    "\n",
    "x_valid = np.array(convert_image_to_array(x_validate))\n",
    "print('Validation set shape : ',x_valid.shape)\n",
    "\n",
    "x_test = np.array(convert_image_to_array(x_test))\n",
    "print('Test set shape : ',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255\n",
    "x_valid = x_valid.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "y_validate = y_validate.reshape(y_validate.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "classes = ['R','O']\n",
    "for i in range(1,26):\n",
    "    index = np.random.randint(x_train.shape[0])\n",
    "    plt.subplot(5, 5, i)\n",
    "    plt.imshow(np.squeeze(x_train[index]), cmap='cool')\n",
    "    plt.title(classes[int(y_train[index])])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(3, 3),kernel_initializer='he_normal',activation='relu',input_shape=(100,100,3),name = 'conv0'))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', name = 'conv1'))\n",
    "model.add(BatchNormalization(name='bn0'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name = 'maxpool0'))\n",
    "model.add(Dropout(0.2,name='dropout0'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',name = 'conv2'))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', name = 'conv3'))\n",
    "model.add(BatchNormalization(name='bn1'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name = 'maxpool1'))\n",
    "model.add(Dropout(0.2,name='dropout1'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',name = 'conv4'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3),activation='relu',name = 'conv5'))\n",
    "model.add(BatchNormalization(name='bn2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name = 'maxpool2'))\n",
    "model.add(Dropout(0.3,name='dropout2'))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',name = 'conv6'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3),activation='relu',name = 'conv7'))\n",
    "model.add(BatchNormalization(name='bn3'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name = 'maxpool3'))\n",
    "model.add(Dropout(0.3,name='dropout3'))\n",
    "\n",
    "model.add(Flatten(name='fc'))\n",
    "model.add(Dense(512, activation='relu',name = 'Dense0'))\n",
    "model.add(Dense(256, activation='relu',name = 'Dense1'))\n",
    "model.add(Dense(128, activation='relu',name = 'Dense2'))\n",
    "model.add(Dropout(0.3,name='dropout4'))\n",
    "model.add(Dense(2, activation='softmax',name = 'Dense3'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'cnn.hdf5', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 15, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReduceLR = ReduceLROnPlateau(patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [earlystop, checkpoint, ReduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 32), epochs = 60, verbose=1,callbacks = callbacks,validation_data=(x_valid,y_validate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"Trained_cnn_history.pickle\",\"wb\")\n",
    "pickle.dump(history.history, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"Trained_cnn_history.pickle\",\"rb\")\n",
    "saved_history = pickle.load(pickle_in)\n",
    "print(saved_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validate'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('cnn.hdf5')\n",
    "model.load_weights('cnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss :',score[0])\n",
    "print('Test Accuracy :',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions for the test data\n",
    "predicted_classes = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = confusion_matrix(y_test, predicted_classes) \n",
    "\n",
    "plt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('confusion_matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['R','O'], rotation=90)\n",
    "plt.yticks(tick_marks, ['R','O'])\n",
    "#Following is to mention the predicated numbers in the plot and highligh the numbers the most predicted number for particular label\n",
    "thresh = confusion_mtx.max() / 2.\n",
    "for i, j in itertools.product(range(confusion_mtx.shape[0]), range(confusion_mtx.shape[1])):\n",
    "    plt.text(j, i, confusion_mtx[i, j],\n",
    "    horizontalalignment=\"center\",\n",
    "    color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = confusion_matrix(y_test, predicted_classes) \n",
    "\n",
    "plt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('confusion_matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(1)\n",
    "plt.xticks(tick_marks, ['R','O'],rotation=90)\n",
    "plt.yticks(tick_marks, ['R','O'])\n",
    "thresh = confusion_mtx.max() / 2.\n",
    "for i, j in itertools.product(range(confusion_mtx.shape[0]), range(confusion_mtx.shape[1])):\n",
    "    plt.text(j, i, confusion_mtx[i, j],\n",
    "    horizontalalignment=\"center\",\n",
    "    color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
